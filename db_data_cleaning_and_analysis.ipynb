{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import all modules and classes from other files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px \n",
    "import seaborn as sns\n",
    "from datetime import datetime \n",
    "from db_datatransform import DataTransform\n",
    "from db_info import DataFrameInfo\n",
    "from db_info import read_csv\n",
    "from db_clean_data import Plotter\n",
    "from db_clean_data import DataFrameTransform\n",
    "from db_datatransform import DataTransform\n",
    "from db_info import DataFrameInfo\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'loan_payments.csv'\n",
    "loan_payments_df = read_csv(filename)  # calls the read_csv funciton to load data \n",
    "loan_payments_df = loan_payments_df.drop(columns=loan_payments_df.columns[0], axis=1) # remove additional index column\n",
    "column_names = loan_payments_df.columns.tolist() # creates a list of the column headings as strings \n",
    "data_transform_instance = DataTransform(loan_payments_df) # creates an instance of the DataTransform class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_convert_to_datetime = ['issue_date', \n",
    "                                 'earliest_credit_line', \n",
    "                                 'last_payment_date',\n",
    "                                 'next_payment_date', \n",
    "                                 'last_credit_pull_date'] # list of strings specifying columns to be converted\n",
    "col_to_convert_to_float = ['term','employment_length'] # list of strings of column names\n",
    "col_to_convert_to_str = ['grade', 'sub_grade'] # list of strings of column names\n",
    "col_to_convert_to_categorical = ['home_ownership',\n",
    "                                    'verification_status', \n",
    "                                    'loan_status', \n",
    "                                    'payment_plan', \n",
    "                                    'purpose', \n",
    "                                    'application_type']  # list of strings of column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(col_to_convert_to_datetime)): # loops over list of column names\n",
    "       datetime_format = '%b-%Y'\n",
    "       data_transform_instance.obj_to_datetime(col_to_convert_to_datetime[i],datetime_format) # calls the obj_to_datetime method to convert data to datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(col_to_convert_to_float)): # loops over list of column names\n",
    "       data_transform_instance.obj_to_int(col_to_convert_to_float[i]) # calls the obj_to_int method to convert data to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(col_to_convert_to_str)): # loops over list of column names\n",
    "       data_transform_instance.obj_to_str(col_to_convert_to_str[i]) # calls the obj_to_string method to convert data to str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(col_to_convert_to_categorical)): #loops over list of column names\n",
    "       data_transform_instance.obj_to_cat(col_to_convert_to_categorical[i]) # calls the obj_to_ method to convert data to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 0.0\n",
      "member_id 0.0\n",
      "loan_amount 0.0\n",
      "funded_amount 5.544799100145673\n",
      "funded_amount_inv 0.0\n",
      "term 8.799395179878667\n",
      "int_rate 9.53144880234552\n",
      "instalment 0.0\n",
      "grade 0.0\n",
      "sub_grade 0.0\n",
      "employment_length 3.905515295679593\n",
      "home_ownership 0.0\n",
      "annual_inc 0.0\n",
      "verification_status 0.0\n",
      "issue_date 0.0\n",
      "loan_status 0.0\n",
      "payment_plan 0.0\n",
      "purpose 0.0\n",
      "dti 0.0\n",
      "delinq_2yrs 0.0\n",
      "earliest_credit_line 0.0\n",
      "inq_last_6mths 0.0\n",
      "mths_since_last_delinq 57.16656524865852\n",
      "mths_since_last_record 88.60245984768859\n",
      "open_accounts 0.0\n",
      "total_accounts 0.0\n",
      "out_prncp 0.0\n",
      "out_prncp_inv 0.0\n",
      "total_payment 0.0\n",
      "total_payment_inv 0.0\n",
      "total_rec_prncp 0.0\n",
      "total_rec_int 0.0\n",
      "total_rec_late_fee 0.0\n",
      "recoveries 0.0\n",
      "collection_recovery_fee 0.0\n",
      "last_payment_date 0.13460935627224282\n",
      "last_payment_amount 0.0\n",
      "next_payment_date 60.12797108664786\n",
      "last_credit_pull_date 0.012907746491858899\n",
      "collections_12_mths_ex_med 0.09404215301211484\n",
      "mths_since_last_major_derog 86.17211557965001\n",
      "policy_code 0.0\n",
      "application_type 0.0\n"
     ]
    }
   ],
   "source": [
    "nulls_info_instance = DataFrameInfo(loan_payments_df) # initialises an instnce of the class to obtain information on the dataframe\n",
    "null_columns = [] # initialise list of null columns\n",
    "for i in range (0, len(column_names)):\n",
    "    null_pc = nulls_info_instance.null_percentage(column_names[i])\n",
    "    if null_pc > 0.0:\n",
    "        null_columns.append(column_names[i]) # append to list of columns if there are any null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id 0.0\n",
      "member_id 0.0\n",
      "loan_amount 0.0\n",
      "funded_amount 0.0\n",
      "funded_amount_inv 0.0\n",
      "term 0.0\n",
      "int_rate 0.0\n",
      "instalment 0.0\n",
      "grade 0.0\n",
      "sub_grade 0.0\n",
      "employment_length 0.0\n",
      "home_ownership 0.0\n",
      "annual_inc 0.0\n",
      "verification_status 0.0\n",
      "issue_date 0.0\n",
      "loan_status 0.0\n",
      "payment_plan 0.0\n",
      "purpose 0.0\n",
      "dti 0.0\n",
      "delinq_2yrs 0.0\n",
      "earliest_credit_line 0.0\n",
      "inq_last_6mths 0.0\n",
      "mths_since_last_delinq 0.0\n",
      "mths_since_last_record 0.0\n",
      "open_accounts 0.0\n",
      "total_accounts 0.0\n",
      "out_prncp 0.0\n",
      "out_prncp_inv 0.0\n",
      "total_payment 0.0\n",
      "total_payment_inv 0.0\n",
      "total_rec_prncp 0.0\n",
      "total_rec_int 0.0\n",
      "total_rec_late_fee 0.0\n",
      "recoveries 0.0\n",
      "collection_recovery_fee 0.0\n",
      "last_payment_date 0.0\n",
      "last_payment_amount 0.0\n",
      "next_payment_date 0.0\n",
      "last_credit_pull_date 0.0\n",
      "collections_12_mths_ex_med 0.0\n",
      "mths_since_last_major_derog 0.0\n",
      "policy_code 0.0\n",
      "application_type 0.0\n",
      "['funded_amount', 'term', 'int_rate', 'employment_length', 'mths_since_last_delinq', 'mths_since_last_record', 'last_payment_date', 'next_payment_date', 'last_credit_pull_date', 'collections_12_mths_ex_med', 'mths_since_last_major_derog']\n"
     ]
    }
   ],
   "source": [
    "for i in range (0, len(column_names)):\n",
    "    null_pc = nulls_info_instance.null_percentage(column_names[i])\n",
    "    print(column_names[i], null_pc)\n",
    "print(null_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['mths_since_last_delinq', \n",
    "                      'mths_since_last_record', \n",
    "                      'next_payment_date', \n",
    "                      'mths_since_last_major_derog'] # list of strings of columns with > 50% null values\n",
    "columns_to_impute = ['funded_amount','term','int_rate','employment_length'] # list of strings of columns with a small amount of null values \n",
    "columns_to_drop_null_value_rows = ['last_payment_date', \n",
    "                                      'last_credit_pull_date', \n",
    "                                      'collections_12_mths_ex_med'] # list of strings of columns with < 1% null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_payments_df_copy = loan_payments_df.copy() #create copy of df before removing any values for data preservation\n",
    "remove_null_instance = DataFrameTransform(loan_payments_df_copy) # initialises an instance of the DataFrameTransform class for dealing with null values\n",
    "df_info_instance = DataFrameInfo(loan_payments_df_copy) # initialises an instnce of the class to get info on the df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_drop)):\n",
    "      loan_payments_df_copy = remove_null_instance.drop_column(columns_to_drop[i]) # drops columns with > 50 % null values\n",
    "for i in range(0, len(columns_to_drop_null_value_rows)):\n",
    "      loan_payments_df_copy = remove_null_instance.drop_null_rows(columns_to_drop_null_value_rows[i]) # drops rows with null values in specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_impute)):\n",
    "      mean = df_info_instance.get_mean(columns_to_impute[i])\n",
    "      print('MEAN', columns_to_impute[i], mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_impute)):\n",
    "      stdv = df_info_instance.get_stdev(columns_to_impute[i])\n",
    "      print('STDEV', columns_to_impute[i], stdv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_impute)):\n",
    "       mode = df_info_instance.get_mode(columns_to_impute[i])\n",
    "       print('MODE', columns_to_impute[i], mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_impute)):\n",
    "       median = df_info_instance.get_median(columns_to_impute[i])\n",
    "       print('MEDIAN', columns_to_impute[i], median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_impute)):\n",
    "       ranges = df_info_instance.get_range(columns_to_impute[i])\n",
    "       print('RANGE',columns_to_impute[i], ranges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(columns_to_impute)):\n",
    "       df_info_instance.get_normal_dist(columns_to_impute[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_payments_df_copy = remove_null_instance.impute_na_with_mode('term') # impute null values with mode, as this is categorical data\n",
    "loan_payments_df_copy = remove_null_instance.impute_na_with_median('employment_length') # impute null value with median, to keep all values as whole numbers \n",
    "loan_payments_df_copy = remove_null_instance.impute_na_with_mean('funded_amount') # impute null values with mean as data is continuous with a normal distribution \n",
    "loan_payments_df_copy = remove_null_instance.impute_na_with_mean('int_rate') # impute null values with mean as data is continuous with a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names_copy = loan_payments_df_copy.columns.tolist() # creates a list of the remaining column headings as strings \n",
    "null_columns_copy = []\n",
    "for i in range (0, len(column_names_copy)): # to check that the correct columns and rows have been dropped and view null percentages to confirm \n",
    "    null_pc = df_info_instance.null_percentage(column_names_copy[i])\n",
    "    print(column_names_copy[i], null_pc)\n",
    "    if null_pc > 0.0:\n",
    "      null_columns_copy.append(column_names_copy[i]) # append columns with null values to list \n",
    "print(null_columns_copy) # check print to ensure correct columns and rows have been dropped "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotter_instance = Plotter(loan_payments_df_copy) # initialise an instance of the plotter class for data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # visualise data to observe skew\n",
    "for i in range(0, len(column_names_copy)):\n",
    "      plotter_instance.plot_hist(column_names_copy[i])\n",
    "\n",
    "for i in range(0, len(column_names_copy)):\n",
    "      plotter_instance.plot_KDE(column_names_copy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df_skew = loan_payments_df_copy.skew(axis=0,numeric_only = True) # obtain the skew of each numeric column in the dataframe\n",
    "print(loan_df_skew)\n",
    "check_skewed_columns = loan_df_skew.index\n",
    "skewed_columns = []\n",
    "for i in range(0, len(loan_df_skew)): \n",
    "      if loan_df_skew.iloc[i] > 2 or loan_df_skew.iloc[i] < -2: # append columns with a skew < -2 or > 2 to the list\n",
    "         skewed_columns.append(check_skewed_columns[i])\n",
    "print(skewed_columns) # show skewed columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewed_columns_to_ignore = ['id','member_id','delinq_2yrs','inq_last_6mths','collections_12_mths_ex_med'] # skewed columns that do not need to be transformed\n",
    "zero_maj_skewed_columns_to_ignore = ['out_prncp','out_prncp_inv','total_rec_late_fee','collection_recovery_fee'] # list of columns containing a majority of 0 values\n",
    "for i in range(0, len(skewed_columns_to_ignore)):\n",
    "      skewed_columns.remove(skewed_columns_to_ignore[i]) # removes columns that represent IDs/categorical data from list of skewed columns\n",
    "for i in range(0, len(zero_maj_skewed_columns_to_ignore)):\n",
    "      skewed_columns.remove(zero_maj_skewed_columns_to_ignore[i]) # removes columns that contain a majority of 0 values, meaning transformations are not appropriate\n",
    "print(skewed_columns) # check to ensure correct cols are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skew transformations \n",
    "Skewed columns are identified, but no skew transformations are performed to enable querying and analysis of the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_payments_df_transformed = loan_payments_df_copy.copy() # create copy of transformed data with appropriate naming\n",
    "transform_instance = DataFrameTransform(loan_payments_df_transformed) # initialise an instance of the class with the transformed dataframe\n",
    "transformed_plotter_instance = Plotter(loan_payments_df_transformed) # initialise an instance of the plotter class for data visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # visualise data to identify outliers \n",
    "for i in range(0, len(column_names_copy)):\n",
    "      transformed_plotter_instance.plot_hist(column_names_copy[i])\n",
    "\n",
    "for i in range(0, len(column_names_copy)):\n",
    "      transformed_plotter_instance.plot_KDE(column_names_copy[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers observed in visualisation of data \n",
    "columns_with_max_outliers = ['total_rec_late_fee','open_accounts','total_accounts','collection_recovery_fee'] # list of strings of column names for columns with high values to remove\n",
    "for i in range(0, len(columns_with_max_outliers)):\n",
    "      loan_payments_df_transformed = transform_instance.remove_top_val(columns_with_max_outliers[i]) # remove max values\n",
    "# additional to remove highest val again\n",
    "loan_payments_df_transformed = transform_instance.remove_top_val('collection_recovery_fee') \n",
    "loan_payments_df_transformed = transform_instance.remove_top_val('total_rec_late_fee') \n",
    "\n",
    "columns_with_negatives = ['recoveries','last_payment_amount'] # list of strings of column names for columns with -ve values to remove\n",
    "for i in range(0, len(columns_with_negatives)):\n",
    "      loan_payments_df_transformed = transform_instance.remove_negatives(columns_with_negatives[i]) # remove negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_plotter_instance.plot_corr_matrix() # visualise correlated columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated columns\n",
    "Overly correlated columns are identified with a correlation heatmap but no correlated columns are dropped in order to enable querying and analysis of the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_payments_df_transformed.to_pickle('cleaned_data.pickle') # saves data in pickle format to preserve data types/data transformations "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data cleaning end\n",
    "Data cleaning has been completed and the cleaned data saved to a pickle file for data preservation "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
